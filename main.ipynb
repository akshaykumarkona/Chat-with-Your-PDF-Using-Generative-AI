{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOME\\anaconda3\\envs\\conda_env_genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"]=\"enter your gemini-api-key\"\n",
    "\n",
    "gemini_llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", max_output_tokens=350, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(\"C:\\\\Users\\\\HOME\\\\Desktop\\\\GenAI\\\\Projects\\\\Chat-with-your-PDF-using-Langchain-RAG\\\\ml.pdf\")\n",
    "\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "data_splits=splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1fb458a2510>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_embeddings=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "db=Chroma.from_documents(documents=data_splits, embedding=g_embeddings)\n",
    "\n",
    "db"
   ]
  },
 ],
   "source": [
    "# g_embeddings.embed_query(\"What is Machine Learning?\") #Returns vector for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing the ChatPrompt Template\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based only on the provided context. \\nThink step by step before providing a detailed answer. \\nI will tip you $1000 if the user finds the answer helpful. \\n<context>\\n{context}\\n</context>\\nQuestion: {input}'))])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', temperature=0.0, max_output_tokens=350, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001FB44F7EA20>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x000001FB4506D850>, default_metadata=())\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combined_chain=create_stuff_documents_chain(gemini_llm, chat_prompt)\n",
    "\n",
    "combined_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001FB458A2510>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001FB458A2510>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based only on the provided context. \\nThink step by step before providing a detailed answer. \\nI will tip you $1000 if the user finds the answer helpful. \\n<context>\\n{context}\\n</context>\\nQuestion: {input}'))])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', temperature=0.0, max_output_tokens=350, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001FB44F7EA20>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x000001FB4506D850>, default_metadata=())\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriver_chain=create_retrieval_chain(retriever,combined_chain)\n",
    "\n",
    "retriver_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a breakdown of the answer, based on the provided context:\\n\\n**1. Identify the Relevant Section:**\\n\\n* The context focuses on \"Supervised Learning\" and provides a detailed explanation of its concepts. \\n\\n**2. Extract Key Information:**\\n\\n* **Definition:** Supervised learning aims to build models that can make accurate predictions on new, unseen data.\\n* **Goal:** The goal is to achieve good \"generalization\" â€“ the ability of the model to perform well on data it hasn\\'t seen during training.\\n* **Challenges:** Overfitting and underfitting are potential issues that can hinder generalization.\\n* **Importance of Data:** The context emphasizes that having more data is often more beneficial than complex model tuning.\\n\\n**3. Formulate the Answer:**\\n\\nSupervised learning is a type of machine learning where the algorithm learns from labeled data. This means the data includes both inputs (features) and corresponding outputs (labels). The goal of supervised learning is to build a model that can accurately predict the output for new, unseen inputs. This is achieved by training the model on the labeled data and then using it to make predictions on new data. \\n\\n**Key Concepts:**\\n\\n* **Generalization:** The ability of the model to perform well on unseen data.\\n* **Overfitting:** When the model learns the training data too well and performs poorly on new data.\\n* **Underfitting:** When the model is too simple and cannot learn the underlying patterns in the data.\\n\\n**In summary:** Supervised learning is a fundamental approach in machine learning where algorithms learn from labeled data to make predictions on new data. The key is to find a balance between model complexity and the amount of data to'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_from_RAG=retriver_chain.invoke({\"input\":input(\"Please enter your question: \")})\n",
    "\n",
    "# print(response_from_RAG)\n",
    "\n",
    "response_from_RAG['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
